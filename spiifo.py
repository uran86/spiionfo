import streamlit as st
import requests
from bs4 import BeautifulSoup
import trafilatura
import os, pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

st.set_page_config(layout="wide")
model = SentenceTransformer('all-MiniLM-L6-v2')
DB_FILE = "spiinfo_memory.pkl"

def load_db():
    if os.path.exists(DB_FILE):
        with open(DB_FILE, "rb") as f:
            return pickle.load(f)
    return {"texts": [], "embeddings": [], "urls": []}

def save_db(db):
    with open(DB_FILE, "wb") as f:
        pickle.dump(db, f)

db = load_db()

def smart_scrape(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        html = requests.get(url, headers=headers, timeout=10).text

        # FÃ¶rsÃ¶k extrahera relevant artikeltext
        result = trafilatura.extract(html, include_comments=False, include_tables=False)
        if result:
            return result
        else:
            # Fallback: ta med synlig text frÃ¥n <p> och <h> taggar
            soup = BeautifulSoup(html, "html.parser")
            text = " ".join(tag.get_text() for tag in soup.find_all(['p', 'h1', 'h2', 'h3']))
            return text[:5000]
    except Exception as e:
        st.error(f"Kunde inte hÃ¤mta data: {e}")
        return ""

def remember(text, url):
    embedding = model.encode(text)
    db["texts"].append(text)
    db["embeddings"].append(embedding)
    db["urls"].append(url)
    save_db(db)

def semantic_search(query, top_k=5):
    if not db["embeddings"]:
        return []
    query_emb = model.encode(query)
    sims = cosine_similarity([query_emb], db["embeddings"])[0]
    idxs = np.argsort(sims)[-top_k:][::-1]
    return [(db["texts"][i], sims[i], db["urls"][i]) for i in idxs]

# ğŸ§  GUI
st.title("SPIINFO v5 â€“ Smart Web Scraper AI")
menu = st.sidebar.radio("VÃ¤lj", ["ğŸ“¥ HÃ¤mta frÃ¥n webben", "ğŸ” FrÃ¥ga minnet", "ğŸ§¾ Visa allt minne"])

if menu == "ğŸ“¥ HÃ¤mta frÃ¥n webben":
    url = st.text_input("Ange en URL att analysera")
    if st.button("HÃ¤mta & lÃ¤r"):
        if url:
            scraped = smart_scrape(url)
            if scraped:
                st.success("InnehÃ¥ll hÃ¤mtat! FÃ¶rhandsvisning nedan:")
                st.text_area("Utdrag", scraped[:1500])
                remember(scraped, url)
            else:
                st.warning("Kunde inte hÃ¤mta vettigt innehÃ¥ll.")
        else:
            st.warning("Ange en giltig URL.")

elif menu == "ğŸ” FrÃ¥ga minnet":
    query = st.text_input("Vad vill du veta?")
    if query:
        results = semantic_search(query)
        if results:
            for i, (text, score, url) in enumerate(results):
                st.markdown(f"### {i+1}. Likhet: {score:.2f}")
                st.markdown(f"ğŸ”— {url}")
                st.write(text[:1000] + "...")
        else:
            st.info("Inget sparat minne Ã¤n.")

elif menu == "ğŸ§¾ Visa allt minne":
    st.subheader("ğŸ§  SPIINFOs minne")
    for i, (txt, url) in enumerate(zip(db["texts"], db["urls"])):
        st.markdown(f"---\n**{i+1} â€“ {url}**\n{txt[:700]}...")
